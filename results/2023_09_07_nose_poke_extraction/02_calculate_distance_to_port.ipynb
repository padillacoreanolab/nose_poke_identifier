{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c3a02adc9e884466bc8c79db549cc3d2",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": [
     {
      "fromCodePoint": 0,
      "marks": {
       "bold": true,
       "underline": true
      },
      "toCodePoint": 17,
      "type": "marks"
     }
    ]
   },
   "source": [
    "# SLEAP Distance Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4546bee655b14a5dbf393161f1228e60",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Brief 1-2 sentence description of notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cell_id": "03b495cefa6a4798a44c7f2e4c6a3ea7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 21,
    "execution_start": 1691424003626,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Imports of all used packages and libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import savgol_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_repo = git.Repo(\".\", search_parent_directories=True)\n",
    "git_root = git_repo.git.rev_parse(\"--show-toplevel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nancy/projects/nose_poke_identifier'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set('notebook', 'ticks', font_scale=1.2)\n",
    "mpl.rcParams['figure.figsize'] = [15,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d290bac2c17940bfbc0f9296beaf70e5",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Inputs & Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e528ce19c608425292151930d380f49f",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Explanation of each input and where it comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cell_id": "6cf83a5811054461a718a71673d09aab",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 373,
    "execution_start": 1691424003628,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Inputs and Required data loading\n",
    "# input varaible names are in all caps snake case\n",
    "# Whenever an input changes or is used for processing \n",
    "# the vairables are all lower in snake case\n",
    "THORAX_INDEX = 1\n",
    "# TONE_TIMESTAMP_DF = pd.read_csv(\"./proc/rce_tone_timestamp.csv\", index_col=0)\n",
    "# VIDEO_TO_FRAME_AND_SUBJECT_DF = pd.read_excel(\"../../proc/video_to_frame_and_subject.xlsx\")\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF = pd.read_pickle(os.path.join(git_root, \"proc/rce_pilot_2_trodes_metadata.pkl\"))\n",
    "# Remove the .videoTimeStamps.cameraHWSync from the video name\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"video_name\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"video_name\"].apply(lambda x: x.strip(\".videoTimeStamps.cameraHWSync\")).iloc[0]\n",
    "\n",
    "SLEAP_DIR = os.path.join(git_root, \"proc/sleap\") \n",
    "\n",
    "OUTPUT_DIR = r\"./proc\" # where data is saved should always be shown in the inputs\n",
    "MED_PC_WIDTH = 29.5\n",
    "MED_PC_HEIGHT = 24\n",
    "FRAME_RATE = 22\n",
    "WINDOW_SIZE = 25\n",
    "DISTANCE_THRESHOLD = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_dir</th>\n",
       "      <th>video_name</th>\n",
       "      <th>session_path</th>\n",
       "      <th>tone_frames</th>\n",
       "      <th>port_entry_frames</th>\n",
       "      <th>recording</th>\n",
       "      <th>current_subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230612_112630_standard_comp_to_training_D1_s...</td>\n",
       "      <td>20230612_112630_standard_comp_to_training_D1_s...</td>\n",
       "      <td>/nancy/projects/nose_poke_identifier/data/2023...</td>\n",
       "      <td>[[1125, 1324], [3519, 3720], [5815, 6014], [76...</td>\n",
       "      <td>[[192, 248], [389, 405], [916, 929], [929, 948...</td>\n",
       "      <td>[20230612_112630_standard_comp_to_training_D1_...</td>\n",
       "      <td>[1.1, 1.2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         session_dir  \\\n",
       "0  20230612_112630_standard_comp_to_training_D1_s...   \n",
       "\n",
       "                                          video_name  \\\n",
       "0  20230612_112630_standard_comp_to_training_D1_s...   \n",
       "\n",
       "                                        session_path  \\\n",
       "0  /nancy/projects/nose_poke_identifier/data/2023...   \n",
       "\n",
       "                                         tone_frames  \\\n",
       "0  [[1125, 1324], [3519, 3720], [5815, 6014], [76...   \n",
       "\n",
       "                                   port_entry_frames  \\\n",
       "0  [[192, 248], [389, 405], [916, 929], [929, 948...   \n",
       "\n",
       "                                           recording current_subject  \n",
       "0  [20230612_112630_standard_comp_to_training_D1_...      [1.1, 1.2]  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e3ee4891d43a4ac287413afc552ca289",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9ccbf6cc70fd4d379fa29317f733771f",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Describe each output that the notebook creates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fc8e8920a6944918a15fac575cdf6e78",
    "deepnote_cell_type": "text-cell-bullet",
    "formattedRanges": []
   },
   "source": [
    "- Is it a plot or is it data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1e639d4776a84aa9ac8ded2e14fa57db",
    "deepnote_cell_type": "text-cell-bullet",
    "formattedRanges": []
   },
   "source": [
    "- How valuable is the output and why is it valuable or useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8999d19b6b7d4d63bc90f0b0bd9ab085",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9b36cdf08567463082b005cb0dec684b",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Describe what is done to the data here and how inputs are manipulated to generate outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sleap_tracks_from_h5(filename):\n",
    "    \"\"\"\n",
    "    Retrieve pose tracking data (tracks) from a SLEAP-generated h5 file.\n",
    "    \n",
    "    This function is intended for use with Pandas' apply method on columns containing filenames.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    filename : str\n",
    "        Path to the SLEAP h5 file containing pose tracking data.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A transposed version of the 'tracks' dataset in the provided h5 file.\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    df['tracks'] = df['filename_column'].apply(get_sleap_tracks_from_h5)\n",
    "    \n",
    "    \"\"\"\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        return f[\"tracks\"][:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sleap_track_names_from_h5(filename):\n",
    "    \"\"\"\n",
    "    Retrieve the names of tracked features from a SLEAP-generated h5 file.\n",
    "    \n",
    "    This function is intended for use with Pandas' apply method on columns containing filenames.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    filename : str\n",
    "        Path to the SLEAP h5 file containing pose tracking data.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    h5py.Dataset\n",
    "        The 'track_names' dataset in the provided h5 file, representing the names of the tracked features.\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    df['track_names'] = df['filename_column'].apply(get_sleap_track_names_from_h5)\n",
    "    \n",
    "    \"\"\"\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        return [item.tobytes().decode('utf-8') for item in f[\"track_names\"][:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_names_from_sleap(filename):\n",
    "    \"\"\"\n",
    "    Retrieve node names from a SLEAP h5 file.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): Path to the SLEAP h5 file.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: List of node names.\n",
    "    \"\"\"\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        return [n.decode() for n in f[\"node_names\"][:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fill_missing(Y, kind=\"linear\"):\n",
    "    \"\"\"Fills missing values independently along each dimension after the first.\"\"\"\n",
    "\n",
    "    # Store initial shape.\n",
    "    initial_shape = Y.shape\n",
    "\n",
    "    # Flatten after first dim.\n",
    "    Y = Y.reshape((initial_shape[0], -1))\n",
    "\n",
    "    # Interpolate along each slice.\n",
    "    for i in range(Y.shape[-1]):\n",
    "        y = Y[:, i]\n",
    "\n",
    "        # Build interpolant.\n",
    "        x = np.flatnonzero(~np.isnan(y))\n",
    "        f = interp1d(x, y[x], kind=kind, fill_value=np.nan, bounds_error=False)\n",
    "\n",
    "        # Fill missing\n",
    "        xq = np.flatnonzero(np.isnan(y))\n",
    "        y[xq] = f(xq)\n",
    "        \n",
    "        # Fill leading or trailing NaNs with the nearest non-NaN values\n",
    "        mask = np.isnan(y)\n",
    "        y[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), y[~mask])\n",
    "\n",
    "        # Save slice\n",
    "        Y[:, i] = y\n",
    "\n",
    "    # Restore to initial shape.\n",
    "    Y = Y.reshape(initial_shape)\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_velocity(node_loc, window_size=25, polynomial_order=3):\n",
    "    \"\"\"\n",
    "    Calculate the velocity of tracked nodes from pose data.\n",
    "    \n",
    "    The function utilizes the Savitzky-Golay filter to smooth the data and compute the velocity.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    node_loc : numpy.ndarray\n",
    "        The location of nodes, represented as an array of shape [frames, 2]. \n",
    "        Each row represents x and y coordinates for a particular frame.\n",
    "        \n",
    "    window_size : int, optional\n",
    "        The size of the window used for the Savitzky-Golay filter. \n",
    "        Represents the number of consecutive data points used when smoothing the data.\n",
    "        Default is 25.\n",
    "        \n",
    "    polynomial_order : int, optional\n",
    "        The order of the polynomial fit to the data within the Savitzky-Golay filter window.\n",
    "        Default is 3.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        The velocity for each frame, calculated from the smoothed x and y coordinates.\n",
    "    \n",
    "    \"\"\"\n",
    "    node_loc_vel = np.zeros_like(node_loc)\n",
    "    \n",
    "    # For each coordinate (x and y), smooth the data and calculate the derivative (velocity)\n",
    "    for c in range(node_loc.shape[-1]):\n",
    "        node_loc_vel[:, c] = savgol_filter(node_loc[:, c], window_size, polynomial_order, deriv=1)\n",
    "    \n",
    "    # Calculate the magnitude of the velocity vectors for each frame\n",
    "    node_vel = np.linalg.norm(node_loc_vel, axis=1)\n",
    "\n",
    "    return node_vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sleap_data(filename):\n",
    "    \"\"\"\n",
    "    Extracts coordinates, names of body parts, and track names from a SLEAP file.\n",
    "    \n",
    "    Parameters:\n",
    "    - filename (str): Path to the SLEAP file.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: A tuple containing the following elements:\n",
    "        * locations (numpy.ndarray): Array containing the coordinates.\n",
    "        * node_names (list of str): List of body part names.\n",
    "        * track_names (list of str): List of track names.\n",
    "    \n",
    "    Example:\n",
    "    >>> locations, node_names, track_names = extract_sleap_data(\"path/to/sleap/file.h5\")\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        result[\"locations\"] = f[\"tracks\"][:].T\n",
    "        result[\"node_names\"] = [n.decode() for n in f[\"node_names\"][:]]\n",
    "        result[\"track_names\"] = [n.decode() for n in f[\"track_names\"][:]]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_dimension_in_array(arr, dimension=0, ratio=1):\n",
    "    \"\"\"\n",
    "    Rescale values of a specified dimension in a 3D numpy array for the entire array.\n",
    "    \n",
    "    Parameters:\n",
    "    - arr (numpy.ndarray): A 3D numpy array where the third dimension is being rescaled.\n",
    "    - dimension (int, default=0): Specifies which dimension (0 or 1) of the third \n",
    "                                  dimension in the array should be rescaled. \n",
    "                                  For instance, in many contexts:\n",
    "                                  0 represents the x-coordinate, \n",
    "                                  1 represents the y-coordinate.\n",
    "    - ratio (float, default=1): The scaling factor to be applied.\n",
    "    \n",
    "    Returns:\n",
    "    - numpy.ndarray: The rescaled array.\n",
    "    \"\"\"\n",
    "    \n",
    "    arr[:,:,dimension] *= ratio\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_and_zero(arr, frame_start, frame_end):\n",
    "    # Create an array of zeros with the same shape as the input array\n",
    "    result = np.zeros_like(arr)\n",
    "    \n",
    "    # Update the zeros array with the values from the input array slice\n",
    "    result[frame_start:frame_end] = arr[frame_start:frame_end]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the h5 files between recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_dir</th>\n",
       "      <th>video_name</th>\n",
       "      <th>session_path</th>\n",
       "      <th>tone_frames</th>\n",
       "      <th>port_entry_frames</th>\n",
       "      <th>recording</th>\n",
       "      <th>current_subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230612_112630_standard_comp_to_training_D1_s...</td>\n",
       "      <td>20230612_112630_standard_comp_to_training_D1_s...</td>\n",
       "      <td>/nancy/projects/nose_poke_identifier/data/2023...</td>\n",
       "      <td>[[1125, 1324], [3519, 3720], [5815, 6014], [76...</td>\n",
       "      <td>[[192, 248], [389, 405], [916, 929], [929, 948...</td>\n",
       "      <td>[20230612_112630_standard_comp_to_training_D1_...</td>\n",
       "      <td>[1.1, 1.2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         session_dir  \\\n",
       "0  20230612_112630_standard_comp_to_training_D1_s...   \n",
       "\n",
       "                                          video_name  \\\n",
       "0  20230612_112630_standard_comp_to_training_D1_s...   \n",
       "\n",
       "                                        session_path  \\\n",
       "0  /nancy/projects/nose_poke_identifier/data/2023...   \n",
       "\n",
       "                                         tone_frames  \\\n",
       "0  [[1125, 1324], [3519, 3720], [5815, 6014], [76...   \n",
       "\n",
       "                                   port_entry_frames  \\\n",
       "0  [[192, 248], [389, 405], [916, 929], [929, 948...   \n",
       "\n",
       "                                           recording current_subject  \n",
       "0  [20230612_112630_standard_comp_to_training_D1_...      [1.1, 1.2]  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20230612_112630_standard_comp_to_training_D1_subj_1-2_and_1-1.1'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"video_name\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIDEO_TO_FRAME_AND_SUBJECT_DF[\"video_path\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"video_name\"].apply(lambda x: os.path.join(SLEAP_DIR, \"*\", x + \"*.h5\"))\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_glob\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"video_name\"].apply(lambda x: glob.glob(os.path.join(SLEAP_DIR, \"*\", x + \"*2_subj*.h5\")))\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_path\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_glob\"].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nancy/projects/nose_poke_identifier/proc/sleap/20230612_112630_standard_comp_to_training_D1_subj_1-2_and_1-1/20230612_112630_standard_comp_to_training_D1_subj_1-2_and_1-1.1.fixed.2_subj.round_1.id_corrected.h5'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_path\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"all_sleap_data\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_path\"].apply(lambda x: extract_sleap_data(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"locations\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"all_sleap_data\"].apply(lambda x: x[\"locations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"track_names\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"all_sleap_data\"].apply(lambda x: x[\"track_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [1.1, 1.2]\n",
       "Name: track_names, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"track_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the coordinates of all the body parts for all the animals for the entire recording\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"locations\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_path\"].apply(lambda x: get_sleap_tracks_from_h5(x))\n",
    "# Getting the name of the tracks which correspond to the animal id\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"track_names\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_path\"].apply(lambda x: get_sleap_track_names_from_h5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68258, 6, 2, 2)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"locations\"].iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure all the subject IDs are strings instead of floating point numbers\n",
    "# VIDEO_TO_FRAME_AND_SUBJECT_DF[\"individual_subj\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"individual_subj\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [1.1, 1.2]\n",
       "Name: track_names, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"track_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the indexes of each subject from the track list\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_to_index\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: {k: x[\"track_names\"].index(k) for k in x[\"current_subject\"]}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'1.1': 0, '1.2': 1}\n",
       "Name: subject_to_index, dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_to_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_to_tracks\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: {k:v for k, v in x[\"subject_to_index\"].items()}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'1.1': 0, '1.2': 1}\n",
       "Name: subject_to_tracks, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_to_tracks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_to_tracks\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: {k: x[\"locations\"][:,:,:,v] for k, v in x[\"subject_to_index\"].items()}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary items to list of items\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_to_tracks\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_to_tracks\"].apply(lambda x: list(x.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode based on the lists\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF = VIDEO_TO_FRAME_AND_SUBJECT_DF.explode([\"subject_to_tracks\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_dir</th>\n",
       "      <th>video_name</th>\n",
       "      <th>session_path</th>\n",
       "      <th>tone_frames</th>\n",
       "      <th>port_entry_frames</th>\n",
       "      <th>recording</th>\n",
       "      <th>current_subject</th>\n",
       "      <th>video_path</th>\n",
       "      <th>sleap_path</th>\n",
       "      <th>all_sleap_data</th>\n",
       "      <th>locations</th>\n",
       "      <th>track_names</th>\n",
       "      <th>sleap_glob</th>\n",
       "      <th>subject_to_index</th>\n",
       "      <th>subject_to_tracks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230612_112630_standard_comp_to_training_D1_s...</td>\n",
       "      <td>20230612_112630_standard_comp_to_training_D1_s...</td>\n",
       "      <td>/nancy/projects/nose_poke_identifier/data/2023...</td>\n",
       "      <td>[[1125, 1324], [3519, 3720], [5815, 6014], [76...</td>\n",
       "      <td>[[192, 248], [389, 405], [916, 929], [929, 948...</td>\n",
       "      <td>[20230612_112630_standard_comp_to_training_D1_...</td>\n",
       "      <td>[1.1, 1.2]</td>\n",
       "      <td>[/nancy/projects/nose_poke_identifier/proc/sle...</td>\n",
       "      <td>/nancy/projects/nose_poke_identifier/proc/slea...</td>\n",
       "      <td>{'locations': [[[[331.99508667 244.3555603 ]\n",
       " ...</td>\n",
       "      <td>[[[[331.99508667 244.3555603 ], [127.74658203 ...</td>\n",
       "      <td>[1.1, 1.2]</td>\n",
       "      <td>[/nancy/projects/nose_poke_identifier/proc/sle...</td>\n",
       "      <td>{'1.1': 0, '1.2': 1}</td>\n",
       "      <td>(1.1, [[[331.99508667 127.74658203], [307.3127...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230612_112630_standard_comp_to_training_D1_s...</td>\n",
       "      <td>20230612_112630_standard_comp_to_training_D1_s...</td>\n",
       "      <td>/nancy/projects/nose_poke_identifier/data/2023...</td>\n",
       "      <td>[[1125, 1324], [3519, 3720], [5815, 6014], [76...</td>\n",
       "      <td>[[192, 248], [389, 405], [916, 929], [929, 948...</td>\n",
       "      <td>[20230612_112630_standard_comp_to_training_D1_...</td>\n",
       "      <td>[1.1, 1.2]</td>\n",
       "      <td>[/nancy/projects/nose_poke_identifier/proc/sle...</td>\n",
       "      <td>/nancy/projects/nose_poke_identifier/proc/slea...</td>\n",
       "      <td>{'locations': [[[[331.99508667 244.3555603 ]\n",
       " ...</td>\n",
       "      <td>[[[[331.99508667 244.3555603 ], [127.74658203 ...</td>\n",
       "      <td>[1.1, 1.2]</td>\n",
       "      <td>[/nancy/projects/nose_poke_identifier/proc/sle...</td>\n",
       "      <td>{'1.1': 0, '1.2': 1}</td>\n",
       "      <td>(1.2, [[[244.3555603  395.80090332], [247.7836...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         session_dir  \\\n",
       "0  20230612_112630_standard_comp_to_training_D1_s...   \n",
       "1  20230612_112630_standard_comp_to_training_D1_s...   \n",
       "\n",
       "                                          video_name  \\\n",
       "0  20230612_112630_standard_comp_to_training_D1_s...   \n",
       "1  20230612_112630_standard_comp_to_training_D1_s...   \n",
       "\n",
       "                                        session_path  \\\n",
       "0  /nancy/projects/nose_poke_identifier/data/2023...   \n",
       "1  /nancy/projects/nose_poke_identifier/data/2023...   \n",
       "\n",
       "                                         tone_frames  \\\n",
       "0  [[1125, 1324], [3519, 3720], [5815, 6014], [76...   \n",
       "1  [[1125, 1324], [3519, 3720], [5815, 6014], [76...   \n",
       "\n",
       "                                   port_entry_frames  \\\n",
       "0  [[192, 248], [389, 405], [916, 929], [929, 948...   \n",
       "1  [[192, 248], [389, 405], [916, 929], [929, 948...   \n",
       "\n",
       "                                           recording current_subject  \\\n",
       "0  [20230612_112630_standard_comp_to_training_D1_...      [1.1, 1.2]   \n",
       "1  [20230612_112630_standard_comp_to_training_D1_...      [1.1, 1.2]   \n",
       "\n",
       "                                          video_path  \\\n",
       "0  [/nancy/projects/nose_poke_identifier/proc/sle...   \n",
       "1  [/nancy/projects/nose_poke_identifier/proc/sle...   \n",
       "\n",
       "                                          sleap_path  \\\n",
       "0  /nancy/projects/nose_poke_identifier/proc/slea...   \n",
       "1  /nancy/projects/nose_poke_identifier/proc/slea...   \n",
       "\n",
       "                                      all_sleap_data  \\\n",
       "0  {'locations': [[[[331.99508667 244.3555603 ]\n",
       " ...   \n",
       "1  {'locations': [[[[331.99508667 244.3555603 ]\n",
       " ...   \n",
       "\n",
       "                                           locations track_names  \\\n",
       "0  [[[[331.99508667 244.3555603 ], [127.74658203 ...  [1.1, 1.2]   \n",
       "1  [[[[331.99508667 244.3555603 ], [127.74658203 ...  [1.1, 1.2]   \n",
       "\n",
       "                                          sleap_glob      subject_to_index  \\\n",
       "0  [/nancy/projects/nose_poke_identifier/proc/sle...  {'1.1': 0, '1.2': 1}   \n",
       "1  [/nancy/projects/nose_poke_identifier/proc/sle...  {'1.1': 0, '1.2': 1}   \n",
       "\n",
       "                                   subject_to_tracks  \n",
       "0  (1.1, [[[331.99508667 127.74658203], [307.3127...  \n",
       "1  (1.2, [[[244.3555603  395.80090332], [247.7836...  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split tuple of (key, value) into separate columns\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[['subject_id', 'subject_locations']] = pd.DataFrame(VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_to_tracks\"].tolist(), index=VIDEO_TO_FRAME_AND_SUBJECT_DF.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF = VIDEO_TO_FRAME_AND_SUBJECT_DF.drop(columns=[\"subject_to_tracks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.1\n",
       "1    1.2\n",
       "Name: subject_id, dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_locations\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_locations\"].apply(lambda x: fill_missing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[[331.9950866699219, 127.74658203125], [307.3...\n",
       "1    [[[244.35556030273438, 395.8009033203125], [24...\n",
       "Name: subject_locations, dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_locations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_dir</th>\n",
       "      <th>video_name</th>\n",
       "      <th>session_path</th>\n",
       "      <th>tone_frames</th>\n",
       "      <th>port_entry_frames</th>\n",
       "      <th>recording</th>\n",
       "      <th>current_subject</th>\n",
       "      <th>video_path</th>\n",
       "      <th>sleap_path</th>\n",
       "      <th>all_sleap_data</th>\n",
       "      <th>locations</th>\n",
       "      <th>track_names</th>\n",
       "      <th>sleap_glob</th>\n",
       "      <th>subject_to_index</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>subject_locations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230612_112630_standard_comp_to_training_D1_s...</td>\n",
       "      <td>20230612_112630_standard_comp_to_training_D1_s...</td>\n",
       "      <td>/nancy/projects/nose_poke_identifier/data/2023...</td>\n",
       "      <td>[[1125, 1324], [3519, 3720], [5815, 6014], [76...</td>\n",
       "      <td>[[192, 248], [389, 405], [916, 929], [929, 948...</td>\n",
       "      <td>[20230612_112630_standard_comp_to_training_D1_...</td>\n",
       "      <td>[1.1, 1.2]</td>\n",
       "      <td>[/nancy/projects/nose_poke_identifier/proc/sle...</td>\n",
       "      <td>/nancy/projects/nose_poke_identifier/proc/slea...</td>\n",
       "      <td>{'locations': [[[[331.99508667 244.3555603 ]\n",
       " ...</td>\n",
       "      <td>[[[[331.99508667 244.3555603 ], [127.74658203 ...</td>\n",
       "      <td>[1.1, 1.2]</td>\n",
       "      <td>[/nancy/projects/nose_poke_identifier/proc/sle...</td>\n",
       "      <td>{'1.1': 0, '1.2': 1}</td>\n",
       "      <td>1.1</td>\n",
       "      <td>[[[331.9950866699219, 127.74658203125], [307.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230612_112630_standard_comp_to_training_D1_s...</td>\n",
       "      <td>20230612_112630_standard_comp_to_training_D1_s...</td>\n",
       "      <td>/nancy/projects/nose_poke_identifier/data/2023...</td>\n",
       "      <td>[[1125, 1324], [3519, 3720], [5815, 6014], [76...</td>\n",
       "      <td>[[192, 248], [389, 405], [916, 929], [929, 948...</td>\n",
       "      <td>[20230612_112630_standard_comp_to_training_D1_...</td>\n",
       "      <td>[1.1, 1.2]</td>\n",
       "      <td>[/nancy/projects/nose_poke_identifier/proc/sle...</td>\n",
       "      <td>/nancy/projects/nose_poke_identifier/proc/slea...</td>\n",
       "      <td>{'locations': [[[[331.99508667 244.3555603 ]\n",
       " ...</td>\n",
       "      <td>[[[[331.99508667 244.3555603 ], [127.74658203 ...</td>\n",
       "      <td>[1.1, 1.2]</td>\n",
       "      <td>[/nancy/projects/nose_poke_identifier/proc/sle...</td>\n",
       "      <td>{'1.1': 0, '1.2': 1}</td>\n",
       "      <td>1.2</td>\n",
       "      <td>[[[244.35556030273438, 395.8009033203125], [24...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         session_dir  \\\n",
       "0  20230612_112630_standard_comp_to_training_D1_s...   \n",
       "1  20230612_112630_standard_comp_to_training_D1_s...   \n",
       "\n",
       "                                          video_name  \\\n",
       "0  20230612_112630_standard_comp_to_training_D1_s...   \n",
       "1  20230612_112630_standard_comp_to_training_D1_s...   \n",
       "\n",
       "                                        session_path  \\\n",
       "0  /nancy/projects/nose_poke_identifier/data/2023...   \n",
       "1  /nancy/projects/nose_poke_identifier/data/2023...   \n",
       "\n",
       "                                         tone_frames  \\\n",
       "0  [[1125, 1324], [3519, 3720], [5815, 6014], [76...   \n",
       "1  [[1125, 1324], [3519, 3720], [5815, 6014], [76...   \n",
       "\n",
       "                                   port_entry_frames  \\\n",
       "0  [[192, 248], [389, 405], [916, 929], [929, 948...   \n",
       "1  [[192, 248], [389, 405], [916, 929], [929, 948...   \n",
       "\n",
       "                                           recording current_subject  \\\n",
       "0  [20230612_112630_standard_comp_to_training_D1_...      [1.1, 1.2]   \n",
       "1  [20230612_112630_standard_comp_to_training_D1_...      [1.1, 1.2]   \n",
       "\n",
       "                                          video_path  \\\n",
       "0  [/nancy/projects/nose_poke_identifier/proc/sle...   \n",
       "1  [/nancy/projects/nose_poke_identifier/proc/sle...   \n",
       "\n",
       "                                          sleap_path  \\\n",
       "0  /nancy/projects/nose_poke_identifier/proc/slea...   \n",
       "1  /nancy/projects/nose_poke_identifier/proc/slea...   \n",
       "\n",
       "                                      all_sleap_data  \\\n",
       "0  {'locations': [[[[331.99508667 244.3555603 ]\n",
       " ...   \n",
       "1  {'locations': [[[[331.99508667 244.3555603 ]\n",
       " ...   \n",
       "\n",
       "                                           locations track_names  \\\n",
       "0  [[[[331.99508667 244.3555603 ], [127.74658203 ...  [1.1, 1.2]   \n",
       "1  [[[[331.99508667 244.3555603 ], [127.74658203 ...  [1.1, 1.2]   \n",
       "\n",
       "                                          sleap_glob      subject_to_index  \\\n",
       "0  [/nancy/projects/nose_poke_identifier/proc/sle...  {'1.1': 0, '1.2': 1}   \n",
       "1  [/nancy/projects/nose_poke_identifier/proc/sle...  {'1.1': 0, '1.2': 1}   \n",
       "\n",
       "  subject_id                                  subject_locations  \n",
       "0        1.1  [[[331.9950866699219, 127.74658203125], [307.3...  \n",
       "1        1.2  [[[244.35556030273438, 395.8009033203125], [24...  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the coordinates of the corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_path\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each corner file is the in the same folder and has the same basename of the pose tracking file \n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_path\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_path\"].apply(lambda x: \"{}.fixed.corner.h5\".format(x.split(\"fixed\")[0].strip(\".\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_path\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the indexes of each corner location\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_parts\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_path\"].apply(lambda x: get_node_names_from_sleap(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_parts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the coordinates of all the corners\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_path\"].apply(lambda x: get_sleap_tracks_from_h5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing out each corner and creating a dictionary of name to coordinates\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: {part: x[\"corner_to_coordinate\"][:,index,:,:] for index, part in enumerate(x[\"corner_parts\"])}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out all the Nans because there's only one labeled frame\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: {k: v[~np.isnan(v)] for k, v in x[\"corner_to_coordinate\"].items()}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"].iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the distances between corners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting the average width and height so that we can convert pixels to cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the x-coordinates for the width\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"bottom_width\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"].apply(lambda x: x[\"box_bottom_right\"][0] - x[\"box_bottom_left\"][0])\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"top_width\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"].apply(lambda x: x[\"box_top_right\"][0] - x[\"box_top_left\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the y-coordinates for the height\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"right_height\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"].apply(lambda x: x[\"box_bottom_right\"][1] - x[\"box_top_right\"][1])\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"left_height\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"].apply(lambda x: x[\"box_bottom_left\"][1] - x[\"box_top_left\"][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaging the width and height by adding both sides and then getting the mean\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"average_height\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda row: (row[\"right_height\"] + row[\"left_height\"])/2, axis=1)\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"average_width\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda row: (row[\"bottom_width\"] + row[\"top_width\"])/2, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getthing the pixel to cm ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"width_ratio\"] = MED_PC_WIDTH / VIDEO_TO_FRAME_AND_SUBJECT_DF[\"average_width\"]\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"height_ratio\"] = MED_PC_HEIGHT / VIDEO_TO_FRAME_AND_SUBJECT_DF[\"average_height\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"height_ratio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"width_ratio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Pixels to cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_locations\"][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converting the X-dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"rescaled_locations\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: rescale_dimension_in_array(x[\"subject_locations\"], dimension=0, ratio=x[\"width_ratio\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converting the Y-dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"rescaled_locations\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: rescale_dimension_in_array(x[\"rescaled_locations\"], dimension=1, ratio=x[\"height_ratio\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize dictionary column\n",
    "normalized = pd.json_normalize(VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"])\n",
    "\n",
    "# Drop the original column and concat the normalized DataFrame\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF = pd.concat([VIDEO_TO_FRAME_AND_SUBJECT_DF.drop([\"corner_to_coordinate\"], axis=1), normalized], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for corner in VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_parts\"].iloc[0]:\n",
    "    VIDEO_TO_FRAME_AND_SUBJECT_DF[corner] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: [x[corner][0]*x[\"width_ratio\"], x[corner][1]*x[\"height_ratio\"]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"box_bottom_left\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: [x[\"box_bottom_left\"][0]*x[\"width_ratio\"], x[\"box_bottom_left\"][1]*x[\"height_ratio\"]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking over the tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_INDEX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_path\"].iloc[FILE_INDEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_path\"].iloc[FILE_INDEX], \"r\") as f:\n",
    "    dset_names = list(f.keys())\n",
    "    locations = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"rescaled_locations\"].iloc[FILE_INDEX]\n",
    "    node_names = [n.decode() for n in f[\"node_names\"][:]]\n",
    "    \n",
    "print(\"===HDF5 datasets===\")\n",
    "print(dset_names)\n",
    "print()\n",
    "\n",
    "print(\"===locations data shape===\")\n",
    "print(locations.shape)\n",
    "print()\n",
    "\n",
    "print(\"===nodes===\")\n",
    "for i, name in enumerate(node_names):\n",
    "    print(f\"{i}: {name}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thorax_loc = locations[:, THORAX_INDEX, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(thorax_loc[:,0],label='X-coordinates')\n",
    "# Converting to negative so that we can see both x and y track\n",
    "plt.plot(-1*thorax_loc[:,1], label='Y-coordinates')\n",
    "\n",
    "plt.legend(loc=\"center right\")\n",
    "plt.title('Thorax locations')\n",
    "plt.xlabel(\"Time in frames\")\n",
    "plt.ylabel(\"Coordinate Position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(thorax_loc[:,0],thorax_loc[:,1])\n",
    "\n",
    "\n",
    "plt.title('Thorax tracks')\n",
    "plt.xlabel(\"X-Coordinates\")\n",
    "plt.ylabel(\"Y-Coordinates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"rescaled_locations\"].iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering out all the unnecessary pose estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the start and stop of each subject to remove all other frames where the subject(s) are not there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For this example, let's assume we want the slice [:, 1:3], \n",
    "# i.e., the last two columns of every row\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF['sliced_locations'] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda row: slice_and_zero(row[\"rescaled_locations\"], row[\"start_frame\"], row[\"stop_frame\"]), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF['sliced_locations'].iloc[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF['sliced_locations'].iloc[0][-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the distance from thorax to reward port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"all_sleap_data\"].iloc[0][\"node_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"nose_index\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"all_sleap_data\"].apply(lambda x: x[\"node_names\"].index(\"nose\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"nose_coordinates\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: x[\"sliced_locations\"][:,x[\"nose_index\"],:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"nose_coordinates\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"reward_port\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"nose_to_reward_port_distance\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda row: np.linalg.norm(row[\"nose_coordinates\"] - row[\"reward_port\"], axis=1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"nose_to_reward_port_distance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the pose tracking data with the nose poke entry timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_nose_poke_time_and_sleap = TONE_TIMESTAMP_DF.merge(VIDEO_TO_FRAME_AND_SUBJECT_DF, left_on=\"video_file\", right_on='video_name', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removing all nose pokes that were not part of the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_nose_poke_time_and_sleap = merged_nose_poke_time_and_sleap[(merged_nose_poke_time_and_sleap[\"start_frame\"] <= merged_nose_poke_time_and_sleap[\"video_frame\"]) & (merged_nose_poke_time_and_sleap[\"video_frame\"] <= merged_nose_poke_time_and_sleap[\"stop_frame\"])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to get distance of nose to reward port for each nose poke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_nose_poke_time_and_sleap[\"nose_poke_distance\"] = merged_nose_poke_time_and_sleap.apply(lambda x: x[\"nose_to_reward_port_distance\"][x[\"video_frame\"]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filtering by distance that's below a given threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_nose_poke_time_and_sleap = merged_nose_poke_time_and_sleap[merged_nose_poke_time_and_sleap[\"nose_poke_distance\"] <= DISTANCE_THRESHOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_nose_poke_time_and_sleap[\"video_frame\"].head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_nose_poke_time_and_sleap[\"subject_id\"].head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_nose_poke_time_and_sleap[\"nose_poke_distance\"].head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "cf8fe3695d074ee7887fdf6459cbf5ce",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
