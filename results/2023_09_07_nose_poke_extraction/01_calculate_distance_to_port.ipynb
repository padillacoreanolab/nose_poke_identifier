{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c3a02adc9e884466bc8c79db549cc3d2",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": [
     {
      "fromCodePoint": 0,
      "marks": {
       "bold": true,
       "underline": true
      },
      "toCodePoint": 17,
      "type": "marks"
     }
    ]
   },
   "source": [
    "# SLEAP Distance Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4546bee655b14a5dbf393161f1228e60",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Brief 1-2 sentence description of notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "03b495cefa6a4798a44c7f2e4c6a3ea7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 21,
    "execution_start": 1691424003626,
    "source_hash": null
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79119/2820620140.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Imports of all used packages and libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import savgol_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_repo = git.Repo(\".\", search_parent_directories=True)\n",
    "git_root = git_repo.git.rev_parse(\"--show-toplevel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set('notebook', 'ticks', font_scale=1.2)\n",
    "mpl.rcParams['figure.figsize'] = [15,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d290bac2c17940bfbc0f9296beaf70e5",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Inputs & Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e528ce19c608425292151930d380f49f",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Explanation of each input and where it comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6cf83a5811054461a718a71673d09aab",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 373,
    "execution_start": 1691424003628,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Inputs and Required data loading\n",
    "# input varaible names are in all caps snake case\n",
    "# Whenever an input changes or is used for processing \n",
    "# the vairables are all lower in snake case\n",
    "THORAX_INDEX = 1\n",
    "# TONE_TIMESTAMP_DF = pd.read_csv(\"./proc/rce_tone_timestamp.csv\", index_col=0)\n",
    "# VIDEO_TO_FRAME_AND_SUBJECT_DF = pd.read_excel(\"../../proc/video_to_frame_and_subject.xlsx\")\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF = pd.read_pickle(os.path.join(git_root, \"proc/rce_pilot_2_trodes_metadata.pkl\"))\n",
    "# Remove the .videoTimeStamps.cameraHWSync from the video name\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"video_name\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"video_name\"].apply(lambda x: x.strip(\".videoTimeStamps.cameraHWSync\")).iloc[0]\n",
    "\n",
    "SLEAP_DIR = os.path.join(git_root, \"proc/sleap\") \n",
    "\n",
    "OUTPUT_DIR = r\"./proc\" # where data is saved should always be shown in the inputs\n",
    "MED_PC_WIDTH = 29.5\n",
    "MED_PC_HEIGHT = 24\n",
    "FRAME_RATE = 22\n",
    "WINDOW_SIZE = 25\n",
    "DISTANCE_THRESHOLD = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e3ee4891d43a4ac287413afc552ca289",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9ccbf6cc70fd4d379fa29317f733771f",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Describe each output that the notebook creates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fc8e8920a6944918a15fac575cdf6e78",
    "deepnote_cell_type": "text-cell-bullet",
    "formattedRanges": []
   },
   "source": [
    "- Is it a plot or is it data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1e639d4776a84aa9ac8ded2e14fa57db",
    "deepnote_cell_type": "text-cell-bullet",
    "formattedRanges": []
   },
   "source": [
    "- How valuable is the output and why is it valuable or useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8999d19b6b7d4d63bc90f0b0bd9ab085",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9b36cdf08567463082b005cb0dec684b",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Describe what is done to the data here and how inputs are manipulated to generate outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sleap_tracks_from_h5(filename):\n",
    "    \"\"\"\n",
    "    Retrieve pose tracking data (tracks) from a SLEAP-generated h5 file.\n",
    "    \n",
    "    This function is intended for use with Pandas' apply method on columns containing filenames.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    filename : str\n",
    "        Path to the SLEAP h5 file containing pose tracking data.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A transposed version of the 'tracks' dataset in the provided h5 file.\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    df['tracks'] = df['filename_column'].apply(get_sleap_tracks_from_h5)\n",
    "    \n",
    "    \"\"\"\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        return f[\"tracks\"][:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sleap_track_names_from_h5(filename):\n",
    "    \"\"\"\n",
    "    Retrieve the names of tracked features from a SLEAP-generated h5 file.\n",
    "    \n",
    "    This function is intended for use with Pandas' apply method on columns containing filenames.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    filename : str\n",
    "        Path to the SLEAP h5 file containing pose tracking data.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    h5py.Dataset\n",
    "        The 'track_names' dataset in the provided h5 file, representing the names of the tracked features.\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    df['track_names'] = df['filename_column'].apply(get_sleap_track_names_from_h5)\n",
    "    \n",
    "    \"\"\"\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        return [item.tobytes().decode('utf-8') for item in f[\"track_names\"][:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_names_from_sleap(filename):\n",
    "    \"\"\"\n",
    "    Retrieve node names from a SLEAP h5 file.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): Path to the SLEAP h5 file.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: List of node names.\n",
    "    \"\"\"\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        return [n.decode() for n in f[\"node_names\"][:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fill_missing(Y, kind=\"linear\"):\n",
    "    \"\"\"Fills missing values independently along each dimension after the first.\"\"\"\n",
    "\n",
    "    # Store initial shape.\n",
    "    initial_shape = Y.shape\n",
    "\n",
    "    # Flatten after first dim.\n",
    "    Y = Y.reshape((initial_shape[0], -1))\n",
    "\n",
    "    # Interpolate along each slice.\n",
    "    for i in range(Y.shape[-1]):\n",
    "        y = Y[:, i]\n",
    "\n",
    "        # Build interpolant.\n",
    "        x = np.flatnonzero(~np.isnan(y))\n",
    "        f = interp1d(x, y[x], kind=kind, fill_value=np.nan, bounds_error=False)\n",
    "\n",
    "        # Fill missing\n",
    "        xq = np.flatnonzero(np.isnan(y))\n",
    "        y[xq] = f(xq)\n",
    "        \n",
    "        # Fill leading or trailing NaNs with the nearest non-NaN values\n",
    "        mask = np.isnan(y)\n",
    "        y[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), y[~mask])\n",
    "\n",
    "        # Save slice\n",
    "        Y[:, i] = y\n",
    "\n",
    "    # Restore to initial shape.\n",
    "    Y = Y.reshape(initial_shape)\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_velocity(node_loc, window_size=25, polynomial_order=3):\n",
    "    \"\"\"\n",
    "    Calculate the velocity of tracked nodes from pose data.\n",
    "    \n",
    "    The function utilizes the Savitzky-Golay filter to smooth the data and compute the velocity.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    node_loc : numpy.ndarray\n",
    "        The location of nodes, represented as an array of shape [frames, 2]. \n",
    "        Each row represents x and y coordinates for a particular frame.\n",
    "        \n",
    "    window_size : int, optional\n",
    "        The size of the window used for the Savitzky-Golay filter. \n",
    "        Represents the number of consecutive data points used when smoothing the data.\n",
    "        Default is 25.\n",
    "        \n",
    "    polynomial_order : int, optional\n",
    "        The order of the polynomial fit to the data within the Savitzky-Golay filter window.\n",
    "        Default is 3.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        The velocity for each frame, calculated from the smoothed x and y coordinates.\n",
    "    \n",
    "    \"\"\"\n",
    "    node_loc_vel = np.zeros_like(node_loc)\n",
    "    \n",
    "    # For each coordinate (x and y), smooth the data and calculate the derivative (velocity)\n",
    "    for c in range(node_loc.shape[-1]):\n",
    "        node_loc_vel[:, c] = savgol_filter(node_loc[:, c], window_size, polynomial_order, deriv=1)\n",
    "    \n",
    "    # Calculate the magnitude of the velocity vectors for each frame\n",
    "    node_vel = np.linalg.norm(node_loc_vel, axis=1)\n",
    "\n",
    "    return node_vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sleap_data(filename):\n",
    "    \"\"\"\n",
    "    Extracts coordinates, names of body parts, and track names from a SLEAP file.\n",
    "    \n",
    "    Parameters:\n",
    "    - filename (str): Path to the SLEAP file.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: A tuple containing the following elements:\n",
    "        * locations (numpy.ndarray): Array containing the coordinates.\n",
    "        * node_names (list of str): List of body part names.\n",
    "        * track_names (list of str): List of track names.\n",
    "    \n",
    "    Example:\n",
    "    >>> locations, node_names, track_names = extract_sleap_data(\"path/to/sleap/file.h5\")\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        result[\"locations\"] = f[\"tracks\"][:].T\n",
    "        result[\"node_names\"] = [n.decode() for n in f[\"node_names\"][:]]\n",
    "        result[\"track_names\"] = [n.decode() for n in f[\"track_names\"][:]]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_dimension_in_array(arr, dimension=0, ratio=1):\n",
    "    \"\"\"\n",
    "    Rescale values of a specified dimension in a 3D numpy array for the entire array.\n",
    "    \n",
    "    Parameters:\n",
    "    - arr (numpy.ndarray): A 3D numpy array where the third dimension is being rescaled.\n",
    "    - dimension (int, default=0): Specifies which dimension (0 or 1) of the third \n",
    "                                  dimension in the array should be rescaled. \n",
    "                                  For instance, in many contexts:\n",
    "                                  0 represents the x-coordinate, \n",
    "                                  1 represents the y-coordinate.\n",
    "    - ratio (float, default=1): The scaling factor to be applied.\n",
    "    \n",
    "    Returns:\n",
    "    - numpy.ndarray: The rescaled array.\n",
    "    \"\"\"\n",
    "    \n",
    "    arr[:,:,dimension] *= ratio\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_and_zero(arr, frame_start, frame_end):\n",
    "    # Create an array of zeros with the same shape as the input array\n",
    "    result = np.zeros_like(arr)\n",
    "    \n",
    "    # Update the zeros array with the values from the input array slice\n",
    "    result[frame_start:frame_end] = arr[frame_start:frame_end]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the h5 files between recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"video_name\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIDEO_TO_FRAME_AND_SUBJECT_DF[\"video_path\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"video_name\"].apply(lambda x: os.path.join(SLEAP_DIR, \"*\", x + \"*.h5\"))\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_glob\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"video_name\"].apply(lambda x: glob.glob(os.path.join(SLEAP_DIR, \"*\", x + \"*2_subj*.h5\")))\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_path\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_glob\"].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_path\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"all_sleap_data\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_path\"].apply(lambda x: extract_sleap_data(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"locations\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"all_sleap_data\"].apply(lambda x: x[\"locations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"track_names\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"all_sleap_data\"].apply(lambda x: x[\"track_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"track_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the coordinates of all the body parts for all the animals for the entire recording\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"locations\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_path\"].apply(lambda x: get_sleap_tracks_from_h5(x))\n",
    "# Getting the name of the tracks which correspond to the animal id\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"track_names\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_path\"].apply(lambda x: get_sleap_track_names_from_h5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"locations\"].iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"track_names\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the indexes of each subject from the track list\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_to_index\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: {k: x[\"track_names\"].index(k) for k in x[\"current_subject\"]}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_to_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_to_tracks\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: {k:v for k, v in x[\"subject_to_index\"].items()}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_to_tracks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_to_tracks\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: {k: x[\"locations\"][:,:,:,v] for k, v in x[\"subject_to_index\"].items()}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_to_tracks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the coordinates of the corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_path\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each corner file is the in the same folder and has the same basename of the pose tracking file \n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_path\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_path\"].apply(lambda x: \"{}.fixed.corner.h5\".format(x.split(\"fixed\")[0].strip(\".\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_path\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the indexes of each corner location\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_parts\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_path\"].apply(lambda x: get_node_names_from_sleap(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_parts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the coordinates of all the corners\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_path\"].apply(lambda x: get_sleap_tracks_from_h5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing out each corner and creating a dictionary of name to coordinates\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: {part: x[\"corner_to_coordinate\"][:,index,:,:] for index, part in enumerate(x[\"corner_parts\"])}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out all the Nans because there's only one labeled frame\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: {k: v[~np.isnan(v)] for k, v in x[\"corner_to_coordinate\"].items()}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the distances between corners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting the average width and height so that we can convert pixels to cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the x-coordinates for the width\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"bottom_width\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"].apply(lambda x: x[\"box_bottom_right\"][0] - x[\"box_bottom_left\"][0])\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"top_width\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"].apply(lambda x: x[\"box_top_right\"][0] - x[\"box_top_left\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the y-coordinates for the height\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"right_height\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"].apply(lambda x: x[\"box_bottom_right\"][1] - x[\"box_top_right\"][1])\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"left_height\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"].apply(lambda x: x[\"box_bottom_left\"][1] - x[\"box_top_left\"][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaging the width and height by adding both sides and then getting the mean\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"average_height\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda row: (row[\"right_height\"] + row[\"left_height\"])/2, axis=1)\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"average_width\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda row: (row[\"bottom_width\"] + row[\"top_width\"])/2, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getthing the pixel to cm ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"width_ratio\"] = MED_PC_WIDTH / VIDEO_TO_FRAME_AND_SUBJECT_DF[\"average_width\"]\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"height_ratio\"] = MED_PC_HEIGHT / VIDEO_TO_FRAME_AND_SUBJECT_DF[\"average_height\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"height_ratio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"width_ratio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Pixels to cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converting the X-dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"subject_to_tracks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"rescaled_locations\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: {key: fill_missing(rescale_dimension_in_array(value, dimension=0, ratio=x[\"width_ratio\"])) for key, value in x[\"subject_to_tracks\"].items()}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converting the Y-dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"rescaled_locations\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: {key: rescale_dimension_in_array(value, dimension=1, ratio=x[\"height_ratio\"]) for key, value in x[\"rescaled_locations\"].items()}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize dictionary column\n",
    "normalized = pd.json_normalize(VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_to_coordinate\"])\n",
    "\n",
    "# Drop the original column and concat the normalized DataFrame\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF = pd.concat([VIDEO_TO_FRAME_AND_SUBJECT_DF.drop([\"corner_to_coordinate\"], axis=1), normalized], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for corner in VIDEO_TO_FRAME_AND_SUBJECT_DF[\"corner_parts\"].iloc[0]:\n",
    "    VIDEO_TO_FRAME_AND_SUBJECT_DF[corner] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: [x[corner][0]*x[\"width_ratio\"], x[corner][1]*x[\"height_ratio\"]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking over the tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_INDEX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_path\"].iloc[FILE_INDEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sleap_path\"].iloc[FILE_INDEX], \"r\") as f:\n",
    "    dset_names = list(f.keys())\n",
    "    current_subject = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"current_subject\"].iloc[FILE_INDEX][0]\n",
    "    locations = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"rescaled_locations\"].iloc[FILE_INDEX][current_subject]\n",
    "    node_names = [n.decode() for n in f[\"node_names\"][:]]\n",
    "    \n",
    "print(\"===HDF5 datasets===\")\n",
    "print(dset_names)\n",
    "print()\n",
    "\n",
    "print(\"===locations data shape===\")\n",
    "print(locations.shape)\n",
    "print()\n",
    "\n",
    "print(\"===nodes===\")\n",
    "for i, name in enumerate(node_names):\n",
    "    print(f\"{i}: {name}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thorax_loc = locations[:, THORAX_INDEX, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(thorax_loc[:,0],label='X-coordinates')\n",
    "# Converting to negative so that we can see both x and y track\n",
    "plt.plot(-1*thorax_loc[:,1], label='Y-coordinates')\n",
    "\n",
    "plt.legend(loc=\"center right\")\n",
    "plt.title('Thorax locations')\n",
    "plt.xlabel(\"Time in frames\")\n",
    "plt.ylabel(\"Coordinate Position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(thorax_loc[:,0],thorax_loc[:,1])\n",
    "\n",
    "\n",
    "plt.title('Thorax tracks')\n",
    "plt.xlabel(\"X-Coordinates\")\n",
    "plt.ylabel(\"Y-Coordinates\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering out all the unnecessary pose estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the start and stop of each subject to remove all other frames where the subject(s) are not there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF = VIDEO_TO_FRAME_AND_SUBJECT_DF.drop([\"sleap_glob\", \"subject_to_index\", \"subject_to_tracks\", \"corner_parts\", \"corner_to_coordinate\", \"bottom_width\", \"top_width\", \"right_height\", \"left_height\", \"average_height\", \"average_width\", \"width_ratio\", \"height_ratio\", 'box_top_left', 'box_top_right',\n",
    "       'box_bottom_left', 'box_bottom_right', 'locations', 'current_subject', 'track_names', 'sleap_path', 'corner_path'], errors=\"ignore\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the location of the nose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"all_sleap_data\"].iloc[0][\"node_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the index of the nose\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"nose_index\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"all_sleap_data\"].apply(lambda x: x[\"node_names\"].index(\"nose\"))\n",
    "# Dropping to save memory\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF = VIDEO_TO_FRAME_AND_SUBJECT_DF.drop(columns=[\"all_sleap_data\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a row for each port entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame and 'list_column' is the column with the lists\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF = VIDEO_TO_FRAME_AND_SUBJECT_DF.explode('port_entry_frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"entry_length\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"port_entry_frames\"].apply(lambda x: x[1] - x[0])\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF = VIDEO_TO_FRAME_AND_SUBJECT_DF[VIDEO_TO_FRAME_AND_SUBJECT_DF[\"entry_length\"] >= 1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sliced_locations\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: {key: value[x[\"port_entry_frames\"][0]: x[\"port_entry_frames\"][1]] for key, value in x[\"rescaled_locations\"].items()}, axis=1)\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF = VIDEO_TO_FRAME_AND_SUBJECT_DF.drop(columns=[\"rescaled_locations\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"sliced_locations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the coordinates of the nose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"nose_coordinates\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: {key: value[:,x[\"nose_index\"],:] for key, value in x[\"sliced_locations\"].items()}, axis=1)\n",
    "                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the distance from thorax to reward port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the distance between the nose and the reward port for the first frame during the port entry\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"nose_to_reward_port_distance\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda row: {key: np.linalg.norm(value - row[\"reward_port\"], axis=1)[0] for key, value in row[\"nose_coordinates\"].items()}, axis=1)\n",
    "                                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"nose_to_reward_port_distance\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing which mouse is closer to the port and labeling contested port entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"entry_winner\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF.apply(lambda x: min(x[\"nose_to_reward_port_distance\"], key=x[\"nose_to_reward_port_distance\"].get), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling trials as contested if the distance for both animals is reward port is small\n",
    "# NOTE: change the threshold if needed\n",
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"contested\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"nose_to_reward_port_distance\"].apply(lambda x: all((value) <= 1.5 for value in x.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[VIDEO_TO_FRAME_AND_SUBJECT_DF[\"contested\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[VIDEO_TO_FRAME_AND_SUBJECT_DF[\"contested\"]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[\"port_entry_start\"] = VIDEO_TO_FRAME_AND_SUBJECT_DF[\"port_entry_frames\"].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[VIDEO_TO_FRAME_AND_SUBJECT_DF[\"port_entry_start\"] <= 32134].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TO_FRAME_AND_SUBJECT_DF[(VIDEO_TO_FRAME_AND_SUBJECT_DF[\"port_entry_start\"] <= 32134) & (VIDEO_TO_FRAME_AND_SUBJECT_DF[\"contested\"])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "cf8fe3695d074ee7887fdf6459cbf5ce",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
